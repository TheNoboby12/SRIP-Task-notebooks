{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task#3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HKX9809gxBVp_QgeTNJ5bkeXMw8X4CS8",
      "authorship_tag": "ABX9TyMcN4mWOwuUErH7sDinVIbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheNoboby12/SRIP-Task-notebooks/blob/main/Task_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement two hidden layers neural network classifier from scratch in JAX \n",
        "[20 Marks] <br>\n",
        "<br>\n",
        "● Two hidden layers here means (input - hidden1 - hidden2 - output).\n",
        "\n",
        "● You must not use flax, optax, or any other library for this task.\n",
        "\n",
        "● Use MNIST dataset with 80:20 train:test split.\n",
        "\n",
        "● Manually optimize the number of neurons in hidden layers.\n",
        "\n",
        "● Use gradient descent from scratch to optimize your network. You should use the Pytree concept of JAX to do this elegantly.\n",
        "\n",
        "● Plot loss v/s iterations curve with matplotlib.\n",
        "\n",
        "● Evaluate the model on test data with various classification metrics and briefly discuss their implications.\n"
      ],
      "metadata": {
        "id": "_Y0ywaIHSPG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Solution**\n",
        " This notebook contain implementation"
      ],
      "metadata": {
        "id": "8T0PIW-3Sy1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all Libraries needed \n",
        "* sklearn\n",
        "* jax\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-rSj4PfDE-7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "# special transform function\n",
        "from jax import grad, jit, vmap, value_and_grad,soft_pmap\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Keb_jh-RmqDI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "I have load MNIST dataset from mnist.npz file. \n",
        "then made a fuction to extract it. \n",
        "it give result "
      ],
      "metadata": {
        "id": "fK_F3zmIFbZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nx-uPuiVSD3T"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_mnist():\n",
        "    with jnp.load(f'/content/drive/MyDrive/mnist.npz') as f:\n",
        "        images, labels = f[\"x_train\"], f[\"y_train\"]\n",
        "    images = images.astype(\"float32\").reshape([-1,28,28,1]) / 255\n",
        "    images = jnp.reshape(images, (images.shape[0], images.shape[1] * images.shape[2]))\n",
        "    labels = jnp.eye(10)[labels]\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a MLP class which stand for MultiLayer Perceptron. It has initial variablle assigned. then it has following fuction for this tasks:\n",
        "1.   forwad Propagation\n",
        "2.   backward Propagation\n",
        "3.   training part\n",
        "4. gradient descent\n",
        "5. sigmoid function\n",
        "6. sigmoid derivatives\n",
        "7. Mean Square error fuction\n"
      ],
      "metadata": {
        "id": "frwmqPqiHiHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "\n",
        "class MLP(object):\n",
        "    \"\"\"A Multilayer Perceptron class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, hidden_layers, num_outputs):\n",
        "        \"\"\"Constructor for the MLP. Takes the number of inputs,\n",
        "            a variable number of hidden layers, and number of outputs\n",
        "        Args:\n",
        "            num_inputs (int): Number of inputs\n",
        "            hidden_layers (list): A list of ints for the hidden layers\n",
        "            num_outputs (int): Number of outputs\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "        key = jax.random.PRNGKey(0)\n",
        "\n",
        "        # create a generic representation of the layers\n",
        "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
        "\n",
        "        # create random connection weights for the layers\n",
        "        weights = []\n",
        "        for i in range(len(layers) - 1):\n",
        "            w = jax.random.uniform(key,shape=(layers[i], layers[i + 1]))\n",
        "            weights.append(w)\n",
        "        self.weights = weights\n",
        "\n",
        "        # save derivatives per layer\n",
        "        derivatives = []\n",
        "        for i in range(len(layers) - 1):\n",
        "            d = jnp.zeros((layers[i], layers[i + 1]))\n",
        "            derivatives.append(d)\n",
        "        self.derivatives = derivatives\n",
        "\n",
        "        # save activations per layer\n",
        "        activations = []\n",
        "        for i in range(len(layers)):\n",
        "            a = jnp.zeros(layers[i])\n",
        "            activations.append(a)\n",
        "        self.activations = activations\n",
        "\n",
        "\n",
        "    def forward_propagate(self, inputs):\n",
        "        \"\"\"Computes forward propagation of the network based on input signals.\n",
        "        Args:\n",
        "            inputs (ndarray): Input signals\n",
        "        Returns:\n",
        "            activations (ndarray): Output values\n",
        "        \"\"\"\n",
        "\n",
        "        # the input layer activation is just the input itself\n",
        "        activations = inputs\n",
        "\n",
        "        # save the activations for backpropogation\n",
        "        self.activations[0] = activations\n",
        "\n",
        "        # iterate through the network layers\n",
        "        for i, w in enumerate(self.weights):\n",
        "            # calculate matrix multiplication between previous activation and weight matrix\n",
        "            net_inputs = jnp.dot(activations, w)\n",
        "\n",
        "            # apply sigmoid activation function\n",
        "            activations = self._sigmoid(net_inputs)\n",
        "\n",
        "            # save the activations for backpropogation\n",
        "            self.activations[i + 1] = activations\n",
        "\n",
        "        # return output layer activation\n",
        "        return activations\n",
        "\n",
        "\n",
        "    def back_propagate(self, error):\n",
        "        \"\"\"Backpropogates an error signal.\n",
        "        Args:\n",
        "            error (ndarray): The error to backprop.\n",
        "        Returns:\n",
        "            error (ndarray): The final error of the input\n",
        "        \"\"\"\n",
        "\n",
        "        # iterate backwards through the network layers\n",
        "        for i in reversed(range(len(self.derivatives))):\n",
        "\n",
        "            # get activation for previous layer\n",
        "            activations = self.activations[i+1]\n",
        "\n",
        "            # apply sigmoid derivative function\n",
        "            delta = error * self._sigmoid_derivative(activations)\n",
        "\n",
        "            # reshape delta as to have it as a 2d array\n",
        "            delta_re = delta.reshape(delta.shape[0], -1).T\n",
        "\n",
        "            # get activations for current layer\n",
        "            current_activations = self.activations[i]\n",
        "\n",
        "            # reshape activations as to have them as a 2d column matrix\n",
        "            current_activations = current_activations.reshape(current_activations.shape[0],-1)\n",
        "\n",
        "            # save derivative after applying matrix multiplication\n",
        "            self.derivatives[i] = jnp.dot(current_activations, delta_re)\n",
        "\n",
        "            # backpropogate the next error\n",
        "            error = jnp.dot(delta, self.weights[i].T)\n",
        "\n",
        "\n",
        "    def train(self, inputs, targets, epochs, learning_rate):\n",
        "        \"\"\"Trains model running forward prop and backprop\n",
        "        Args:\n",
        "            inputs (ndarray): X\n",
        "            targets (ndarray): Y\n",
        "            epochs (int): Num. epochs we want to train the network for\n",
        "            learning_rate (float): Step to apply to gradient descent\n",
        "        \"\"\"\n",
        "        x,y=[],[]\n",
        "        # now enter the training loop\n",
        "        for i in range(epochs):\n",
        "            sum_errors = 0\n",
        "            # x.append(sum_errors / len(items))\n",
        "            #     y.append(i+1)\n",
        "            # iterate through all the training data\n",
        "            for j, input in enumerate(inputs):\n",
        "                target = targets[j]\n",
        "\n",
        "                # activate the network!\n",
        "                output = self.forward_propagate(input)\n",
        "\n",
        "                error = target - output\n",
        "\n",
        "                self.back_propagate(error)\n",
        "\n",
        "                # now perform gradient descent on the derivatives\n",
        "                # (this will update the weights\n",
        "                self.gradient_descent(learning_rate)\n",
        "                # Get the sampled loss\n",
        "\n",
        "                # loss = self._mse(target,output)\n",
        "                # x.append(self._mse(target,output))\n",
        "                # keep track of the MSE for reporting later\n",
        "                sum_errors += self._mse(target, output)\n",
        "                x.append(sum_errors)\n",
        "                y.append(i+1)\n",
        "               \n",
        "\n",
        "            plt.plot(x,y)\n",
        "            plt.xlabel(\"Loss\")\n",
        "            plt.ylabel(\"Iterartion/Epochs\")\n",
        "            # Epoch complete, report the training error\n",
        "            print(\"Error: {} at epoch {}\".format(sum_errors / len(items), i+1))\n",
        "\n",
        "        \n",
        "      \n",
        "        print(\"Training complete!\")\n",
        "        print(\"=====\")\n",
        "        \n",
        "\n",
        "    def gradient_descent(self, learningRate):\n",
        "        \"\"\"Learns by descending the gradient\n",
        "        Args:\n",
        "            learningRate (float): How fast to learn.\n",
        "        \"\"\"\n",
        "        # update the weights by stepping down the gradient\n",
        "        for i in range(len(self.weights)):\n",
        "            weights = self.weights[i]\n",
        "            # weights += jax.tree_multimap(self.derivatives[i] * learningRate)\n",
        "            # We perform one gradient update\n",
        "            weights = jax.tree_multimap(lambda old, grad: old - learningRate * grad,weights, self.derivatives[i])\n",
        "          \n",
        "\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function\n",
        "        Args:\n",
        "            x (float): Value to be processed\n",
        "        Returns:\n",
        "            y (float): Output\n",
        "        \"\"\"\n",
        "\n",
        "        y = 1.0 / (1 + jnp.exp(-x))\n",
        "        return y\n",
        "\n",
        "\n",
        "    def _sigmoid_derivative(self, x):\n",
        "        \"\"\"Sigmoid derivative function\n",
        "        Args:\n",
        "            x (float): Value to be processed\n",
        "        Returns:\n",
        "            y (float): Output\n",
        "        \"\"\"\n",
        "        return x * (1.0 - x)\n",
        "\n",
        "\n",
        "    def _mse(self, target, output):\n",
        "        \"\"\"Mean Squared Error loss function\n",
        "        Args:\n",
        "            target (ndarray): The ground trut\n",
        "            output (ndarray): The predicted values\n",
        "        Returns:\n",
        "            (float): Output\n",
        "        \"\"\"\n",
        "        return jnp.average((target - output) ** 2)"
      ],
      "metadata": {
        "id": "pfHQ4qIWmVdN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this the Evaluation part of code:-<br>\n",
        "We first traain our model and then test it .<br>\n",
        "data is splited using train_test_split in 80:20 ratio.\n",
        "we call our MLP class assigned values as\n",
        "MLP(number of inputs, number of hidden layer,number of output)"
      ],
      "metadata": {
        "id": "NjCQmXv9IakO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs,labels = get_mnist()\n",
        "X_train, X_test, y_train, y_test = train_test_split(imgs,labels, test_size=0.2)\n",
        "# TF_CPP_MIN_LOG_LEVEL=0\n",
        "# mlp\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    # create a dataset to train a network for the sum operation\n",
        "    items = X_train\n",
        "    targets = y_train\n",
        "    \n",
        "    # create a Multilayer Perceptron with one hidden layer\n",
        "    mlp = MLP(784, [128,64], 10)\n",
        "\n",
        "    # train network\n",
        "    learning_rate=0.0002\n",
        "    epochs = 10\n",
        "    mlp.train(items, targets, epochs, learning_rate)\n",
        "\n",
        "    # # create dummy data\n",
        "    input = X_test \n",
        "    target = y_test\n",
        "\n",
        "    # get a prediction\n",
        "    output = mlp.forward_propagate(input)\n",
        "    print()\n",
        "    print(\"Output =  {}\".format( output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "nAQSG1q7mVSM",
        "outputId": "d1fcf994-eddc-41bc-cf2e-85883edfbd56"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 0.8998313546180725 at epoch 1\n",
            "Error: 0.8998313546180725 at epoch 2\n",
            "Error: 0.8998313546180725 at epoch 3\n",
            "Error: 0.8998313546180725 at epoch 4\n",
            "Error: 0.8998313546180725 at epoch 5\n",
            "Error: 0.8998313546180725 at epoch 6\n",
            "Error: 0.8998313546180725 at epoch 7\n",
            "Error: 0.8998313546180725 at epoch 8\n",
            "Error: 0.8998313546180725 at epoch 9\n",
            "Error: 0.8998313546180725 at epoch 10\n",
            "Training complete!\n",
            "=====\n",
            "\n",
            "Output =  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}